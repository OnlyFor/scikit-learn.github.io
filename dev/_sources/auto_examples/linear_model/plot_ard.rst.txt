
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/linear_model/plot_ard.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_linear_model_plot_ard.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_linear_model_plot_ard.py:


==================================================
Automatic Relevance Determination Regression (ARD)
==================================================

Fit regression model with Bayesian Ridge Regression.

See :ref:`bayesian_ridge_regression` for more information on the regressor.

Compared to the OLS (ordinary least squares) estimator, the coefficient
weights are slightly shifted toward zeros, which stabilises them.

The histogram of the estimated weights is very peaked, as a sparsity-inducing
prior is implied on the weights.

The estimation of the model is done by iteratively maximizing the
marginal log-likelihood of the observations.

We also plot predictions and uncertainties for ARD
for one dimensional regression using polynomial feature expansion.
Note the uncertainty starts going up on the right side of the plot.
This is because these test samples are outside of the range of the training
samples.

.. GENERATED FROM PYTHON SOURCE LINES 26-33

.. code-block:: default


    import numpy as np
    import matplotlib.pyplot as plt
    from scipy import stats

    from sklearn.linear_model import ARDRegression, LinearRegression








.. GENERATED FROM PYTHON SOURCE LINES 34-35

Generating simulated data with Gaussian weights

.. GENERATED FROM PYTHON SOURCE LINES 35-54

.. code-block:: default


    # Parameters of the example
    np.random.seed(0)
    n_samples, n_features = 100, 100
    # Create Gaussian data
    X = np.random.randn(n_samples, n_features)
    # Create weights with a precision lambda_ of 4.
    lambda_ = 4.0
    w = np.zeros(n_features)
    # Only keep 10 weights of interest
    relevant_features = np.random.randint(0, n_features, 10)
    for i in relevant_features:
        w[i] = stats.norm.rvs(loc=0, scale=1.0 / np.sqrt(lambda_))
    # Create noise with a precision alpha of 50.
    alpha_ = 50.0
    noise = stats.norm.rvs(loc=0, scale=1.0 / np.sqrt(alpha_), size=n_samples)
    # Create the target
    y = np.dot(X, w) + noise








.. GENERATED FROM PYTHON SOURCE LINES 55-56

Fit the ARD Regression

.. GENERATED FROM PYTHON SOURCE LINES 56-62

.. code-block:: default

    clf = ARDRegression(compute_score=True)
    clf.fit(X, y)

    ols = LinearRegression()
    ols.fit(X, y)






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style>#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd {color: black;background-color: white;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd pre{padding: 0;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-toggleable {background-color: white;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-estimator:hover {background-color: #d4ebff;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-item {z-index: 1;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-parallel::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-parallel-item:only-child::after {width: 0;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd div.sk-text-repr-fallback {display: none;}</style><div id="sk-f352ee96-7060-40e8-a36d-a7f6cdc456dd" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>Please rerun this cell to show the HTML repr or trust the notebook.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="9c7f9a38-3653-4bbe-af87-d4f5f01e8319" type="checkbox" checked><label for="9c7f9a38-3653-4bbe-af87-d4f5f01e8319" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 63-65

Plot the true weights, the estimated weights, the histogram of the
weights, and predictions with standard deviations

.. GENERATED FROM PYTHON SOURCE LINES 65-120

.. code-block:: default

    plt.figure(figsize=(6, 5))
    plt.title("Weights of the model")
    plt.plot(clf.coef_, color="darkblue", linestyle="-", linewidth=2, label="ARD estimate")
    plt.plot(
        ols.coef_, color="yellowgreen", linestyle=":", linewidth=2, label="OLS estimate"
    )
    plt.plot(w, color="orange", linestyle="-", linewidth=2, label="Ground truth")
    plt.xlabel("Features")
    plt.ylabel("Values of the weights")
    plt.legend(loc=1)

    plt.figure(figsize=(6, 5))
    plt.title("Histogram of the weights")
    plt.hist(clf.coef_, bins=n_features, color="navy", log=True)
    plt.scatter(
        clf.coef_[relevant_features],
        np.full(len(relevant_features), 5.0),
        color="gold",
        marker="o",
        label="Relevant features",
    )
    plt.ylabel("Features")
    plt.xlabel("Values of the weights")
    plt.legend(loc=1)

    plt.figure(figsize=(6, 5))
    plt.title("Marginal log-likelihood")
    plt.plot(clf.scores_, color="navy", linewidth=2)
    plt.ylabel("Score")
    plt.xlabel("Iterations")


    # Plotting some predictions for polynomial regression
    def f(x, noise_amount):
        y = np.sqrt(x) * np.sin(x)
        noise = np.random.normal(0, 1, len(x))
        return y + noise_amount * noise


    degree = 10
    X = np.linspace(0, 10, 100)
    y = f(X, noise_amount=1)
    clf_poly = ARDRegression(threshold_lambda=1e5)
    clf_poly.fit(np.vander(X, degree), y)

    X_plot = np.linspace(0, 11, 25)
    y_plot = f(X_plot, noise_amount=0)
    y_mean, y_std = clf_poly.predict(np.vander(X_plot, degree), return_std=True)
    plt.figure(figsize=(6, 5))
    plt.errorbar(X_plot, y_mean, y_std, color="navy", label="Polynomial ARD", linewidth=2)
    plt.plot(X_plot, y_plot, color="gold", linewidth=2, label="Ground Truth")
    plt.ylabel("Output y")
    plt.xlabel("Feature X")
    plt.legend(loc="lower left")
    plt.show()



.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/linear_model/images/sphx_glr_plot_ard_001.png
         :alt: Weights of the model
         :srcset: /auto_examples/linear_model/images/sphx_glr_plot_ard_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/linear_model/images/sphx_glr_plot_ard_002.png
         :alt: Histogram of the weights
         :srcset: /auto_examples/linear_model/images/sphx_glr_plot_ard_002.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/linear_model/images/sphx_glr_plot_ard_003.png
         :alt: Marginal log-likelihood
         :srcset: /auto_examples/linear_model/images/sphx_glr_plot_ard_003.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/linear_model/images/sphx_glr_plot_ard_004.png
         :alt: plot ard
         :srcset: /auto_examples/linear_model/images/sphx_glr_plot_ard_004.png
         :class: sphx-glr-multi-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.376 seconds)


.. _sphx_glr_download_auto_examples_linear_model_plot_ard.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example


  .. container:: binder-badge

    .. image:: images/binder_badge_logo.svg
      :target: https://mybinder.org/v2/gh/scikit-learn/scikit-learn/main?urlpath=lab/tree/notebooks/auto_examples/linear_model/plot_ard.ipynb
      :alt: Launch binder
      :width: 150 px


  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_ard.py <plot_ard.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_ard.ipynb <plot_ard.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
